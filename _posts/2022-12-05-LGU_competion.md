---
title: LG U+ 아이들나라 경진대회 회고 (최종 리더보드 26위 성능 모델)
date: 2022-12-05 11:30:00 +0800
categories: [경진대회, 추천시스템]
tags: [경진대회 후기, 추천시스템, LG U+, 회고, ALS, LGBM Ranker]
math: true
mermaid: true
pin: true
published: True
---

![image](https://user-images.githubusercontent.com/88483620/205574483-c4ebeae8-f295-42ba-9d88-004be6f7a5dd.png)
_대회 진행 리더보드에서 21위를 했던 이 모델은 최종 리더보드에서 26위의 성적을 냈다._
   
   

## LG U+ 아이들나라 추천 경진대회
---
> 글쓴이 : 패스트캠퍼스 & 중소벤처기업진흥공단 운영 YearDream School 2기 [추천시스템 트랙 실습코치-강하예진]

이 포스트에서는 개인적인 회고와 Rap-up에 더해서, 이 대회에 관심이 많고 후기가 궁금했던 사람들, 추천시스템 경진대회에 대한 팁을 얻고 싶은 사람들을 위해 작성했다. 경진대회나 프로젝트 참여 경력을 통해 그간 느껴왔던 팁들도 함께 녹여낼 예정이다.    
   
    
# 프로젝트 진행 프로세스
가장 먼저, 주어진 4주간의 기간 동안 어떻게 프로젝트를 진행시켜 나갈지 굵직한 목표를 정한다.     
교육과정 특강에서 만났던 송호연님의 팁은 *6주 프로젝트의 경우 대체적으로 아래의 타임라인* 을 따른다고 하셨다. 이번 프로젝트도 비슷한 방식으로 진행했다.     
   
| 시기          | task                                                         |
|:-------------|:------------------------------------------------------------:|
| 초 1~2주      | EDA + rule base model baseline                               |
| 중 1~2주      | evaluation metric 정의/세팅 + pipeline 안정화 + baseline 모델실험  |
| 후 1~2주      | 평가지표 기준 모델 고도화 + 데모페이지                                |    
    
개인적으로 이번 프로젝트를 진행하며 (새삼스럽게도) 크게 느낀 바는, **EDA에 쓰는 시간이 3~4일을 넘겨선 안 된다**는 것이다. 심화 EDA가 부족한 것 같아도, 모든 정보값을 포함한 데이터셋을 만들지 못했더라도, 무조건 다음 단계로 넘어가라. DS는 모든 프로세스와 프로젝트에 애자일 방법론을 적용해야 하기 때문이다.          
또한 의미없는 통계치나 집계를 확인할 바에는 **유저 재구매 패턴** 을 철저히 확인하는 것이 훨씬 도움이 된다.  뒤에서 다시 이야기하겠지만, 바로 이것이 간단한 모델을 리더보드 상위권을 차지하게 만든 핵심이다.    
너무 기본적인 내용이지만, 데이터 정합성 체크도 반드시 선행되어야 한다.    

            
## pipelining과 코드 자동화의 중요성
EDA를 마친 후, 다음으로 중요한 일은 pipelining이다. 사실 난 위의 표와는 조금 다른 의견을 갖고 있는데, rule base model test 전에 이 작업을 우선적으로 해야 한다.      
여기서 말하는 pipelining이란 git repo를 기준으로 [py 파일이나 data 파일]을 어떻게 쌓아 둘 것인지, data load와 evaluation(sample submission)을 고려한 작업환경을 만드는 것이다. 효율적인 실험 환경 구성을 위해서라도 코드 자동화는 반드시 필요하다.       
    
       
          
## '이렇게' 시간낭비하지 마세요   
   
+  결측과 납득할 수 없는 수치가 많아요! / dataset끼리 병합이 원활하지 않아요!   
     
현업자 D님에게 조언을 구한 뒤 내린 결론은, [결측이 많고 납득할 수 없는 feature나 dataset의 경우 어떻게든 의미를 찾아보려 애쓰는 것보다 쿨하게 드랍하고 '의미를 파악할 수 있는 feature'에 집중하는 것이 더 낫다]는 것이다.    

이번 경진대회를 예시로 들면, 이런 식이다.  
: 시청 로그에 연결된 실제 시청 지속시간이 0초가 압도적으로 높다.      
: log time이 어긋나기 때문에, 시청 로그에 관련된 dataset 두 개를 도저히 합칠 수가 없다.     
    
한 줄 더 적어보자면, full dataset을 만들겠다는 생각은 버리는 게 언제나 정신 건강에 좋다.    
       
+  베이스라인 모델을 아무리 개선해도 성능이 안 올라요!    
    
베이스라인은 어디까지나 가이드라인이다. 베이스라인 모델을 짠 분이 어떤 생각과 의도를 가지고 feature를 선택하고 코드를 구성했는지 완벽 파악할 수 없기 때문에, 그것을 붙잡고 질질 끌거나 거기서부터 개선해나갈 바에는 새로 짜는 것이 (하다못해 feature라도 싹 바꿔서 돌리던가) 훨씬 낫다.    
     
     
## 너무나도 중요한 것    
1. 유저 패턴 파악 (통계량을 보는 Edge-case, User traking)      
2. candidate listing (MP는 답을 알고 있다)        
3. train-test case를 나눈 실험환경 구성     
4. parameter test가 가능한 evalution code 구성 (귀찮다고 미루면 후회한다)   
5. 매 실험마다 param & random seed 기록 (notion 표 기능)    
     
기본 중의 기본이지만, 아무리 강조해도 모자라지 않다. 몇 줄은 피눈물로 적었다...   
      
   
# 내 모델의 성공요인 분석
> 이번에 좋은 성적을 낸 모델이 가진 비밀, 그리고 이를 통해 깨달은 점을 적어보려 한다.    
   
최종 리더보드에서 26위의 성능을 가진 모델은 사실 대단한 코딩이나 엄청난 feature engineering이 들어가지 않았다.   
지금도 믿기지 않지만, ipynb 하나에 짠 **단순한 ALS 모델**이다.  
물론 아무렇게나 돌렸다는 것은 아니다. 나름의 통찰과 로직을 적용한 모델이다. 이것이 어떤 특별한 점을 가졌냐 하면...       
   
       
> user의 재구매 경향이 매우 높다는 인사이트를 파악하고, user가 이미 본 영상도 `재추천` 했다.
{: .prompt-tip }    

         
![image](https://user-images.githubusercontent.com/88483620/205581919-41496ce7-a611-46cf-b51c-21c11638307d.png)
_인증 스크린샷 첨부. 최종 리더보드 기준 26위에 랭크되는 점수다._    
   
      
이번 일을 계기로 제대로 '작동하는' 추천시스템의 본질이 무엇인지 뼈아프게 배웠다. 유저의 핵심 행동원리를 파악하면, 간단한 코드로도 한 달을 갈아넣어 짠 복잡한 코드보다 대단한 성능을 낼 수 있다.   
나는 이번 경진대회를 통해 적은 코스트로 최고의 성능을 뽑아내는 추천 핵심 기술을 배워간다.   

+ 해당 모델을 제출하지 않은 이유는, 팀이 함께 구축한 모델이 아닌 나 혼자서 로직을 구상하고 가볍게 돌려본 모델이었기 때문. 이걸 냈다면 우리 팀은 26위를 했을 것이다. (팀 git repo에서 해당 사실을 증명할 수 있다.) 추후 팀 깃허브 링크 첨부 예정   
    
    
     
# 팀 모델의 실패요인 분석  
    
1. 초반 데이터셋 병합에 너무 많은 cost를 썼다. (심지어 나중엔 쓰지도 않았다)   
2. 무슨 짓을 해도 LGBM 성능이 유의미하게 향상되지 않는다면, 쿨하게 버리고 ALS를 가지고 요리했어야 했다.      
3. 데드라인을 정해 두고, 최소 1시간 전부터 최종 제출물 결정을 논의했어야 했다.   
   
들인 시간이 아깝다고 LGBM Ranker를 끝까지 붙잡고 있던 것이 주요한 실패요인이었다. *우리는 끝까지 LGBM으로 ALS 성능을 넘어보려 했다.* (심지어 ALS를 통해 얻은 여러 결과를 앙상블까지 했다.) 지금 돌아보면 현명한 방법이 아님을 알지만 당시에는 해당 작업들에 매몰되었다.     
온갖 candidate 추가와 feature engineering으로도 성능이 유의미하게 개선되지 않았다면, 애초에 방향성이 잘못된 것이니 ALS를 메인으로 잡아 빠르게 노선을 바꿨어야 했다. (하다못해 LGBM으로 재추천을 더 잘 할 방법이라도 궁리했어야 했다)    
      
이게 내가 생각하는 우리 팀의 실패요인이다. 멋진 문을 찾아놓고도 창문으로 들어가 보겠다고 삽질을 한 셈.    
노력도 가끔 배신한다.  중요한 순간에 현명한 결정을 내리지 못한다면 말이다...    
    
     
# Next Stage
일단 나는 이렇게 Rap-Up을 마치고, Kaggle에서 주최하는 OTTO 추천 대회로 넘어간다. 여기서는 더 다양한 베이스 모델들을 돌려보고, 더 세련된 Emsemble, hyper parameter tuning을 시도할 예정이다.     
LGBM Ranker의 논문적인 이해도 조금 더 필요할 것 같다. 여기에 더해 Deep Learning 모델도 적용해보고 싶다.     
다음 대회에서는 더 좋은 성과를 낼 수 있겠다는 확신이 든다. 